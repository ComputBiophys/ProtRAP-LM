{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recommendations for Efficient Usage:\n",
        "\n",
        "Prioritize using CUDA-enabled devices (NVIDIA GPUs) over Google TPUs for better compatibility.\n",
        "\n",
        "Due to network speed limitations:\n",
        "* Uploading large FASTA files containing numerous sequences may fail.\n",
        "* Downloading ZIP archives with extensive results might also fail.\n",
        "\n",
        "Mitigation strategies:\n",
        "* Split large files into smaller batches for processing.\n",
        "* Mount your Google Drive to Colab's working directory and perform uploads/downloads directly from Google Drive."
      ],
      "metadata": {
        "id": "6xcgCZTS78w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!git clone https://github.com/ComputBiophys/ProtRAP-LM.git\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse,csv,sys\n",
        "import os,requests\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nnF\n",
        "import os # 用于检测 Colab TPU 环境\n",
        "from google.colab import drive,files\n",
        "!pip install biopython\n",
        "from Bio import SeqIO\n",
        "from tqdm import tqdm\n",
        "# --- 设备检测 ---\n",
        "device = None\n",
        "device_type = None\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ and os.environ['COLAB_TPU_ADDR']:\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        device = xm.xla_device()\n",
        "        device_type = \"TPU\"\n",
        "        print(\"TPU detected. Using TPU.\")\n",
        "    except ImportError:\n",
        "        print(\"TPU environment detected, but torch_xla is not installed.\")\n",
        "        print(\"Falling back to CPU/GPU check.\")\n",
        "if device is None:\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        device_type = \"GPU\"\n",
        "        print(f\"GPU detected. Using {torch.cuda.get_device_name(0)}\") # 显示 GPU 名称\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_type = \"CPU\"\n",
        "        print(\"No TPU or GPU detected. Using CPU.\")\n",
        "\n",
        "model_path=lambda x:'ProtRAP-LM/models/model_'+str(x)+'.pts'\n",
        "github_url=lambda x:f\"https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_{str(x)}.pts\"\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded file from {url} to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}, You may manually download this one\")\n",
        "\n",
        "for i in range(10):\n",
        "    if not os.path.exists(model_path(i)):\n",
        "        print('Downloading model_'+str(i))\n",
        "        download_file(github_url(i), model_path(i))\n",
        "\n",
        "def fasta_load(fasta_dir):\n",
        "    fp = open(fasta_dir, 'r')\n",
        "    lines = fp.readlines()\n",
        "    fp.close()\n",
        "    sequence = ''\n",
        "    for line in lines[1:]:\n",
        "        sequence = sequence + line.split()[0]\n",
        "    return sequence\n",
        "def weight_MSE_loss(labels,logits,weights=1):\n",
        "    l=(labels-logits)**2\n",
        "    l=l*weights\n",
        "    return torch.sum(l)\n",
        "def focal_loss_softmax(labels,logits):\n",
        "    y_pred=logits\n",
        "    l=-labels*torch.log(y_pred+1e-8)*((1-y_pred)**2)\n",
        "    return torch.sum(l)\n",
        "\n",
        "class MultiScaleCNN(nn.Module):\n",
        "    def __init__(self,input_dim=1280,output_dim=256):#,size=[3,7,11],padding=[1,3,5]):\n",
        "        super().__init__()\n",
        "        self.cnn1=nn.Conv1d(input_dim,output_dim,3,padding=1)\n",
        "        self.cnn2=nn.Conv1d(input_dim,output_dim,5,padding=2)\n",
        "        self.cnn3=nn.Conv1d(input_dim,output_dim,7,padding=3)\n",
        "        self.cnn4=nn.Conv1d(input_dim,output_dim,9,padding=4)\n",
        "    def forward(self,x):\n",
        "        x=x.permute(0,2,1)\n",
        "        x1=self.cnn1(x)\n",
        "        x2=self.cnn2(x)\n",
        "        x3=self.cnn3(x)\n",
        "        x4=self.cnn4(x)\n",
        "        x=torch.cat((x1,x2,x3,x4), -2)\n",
        "        x=x.permute(0,2,1)\n",
        "        return x\n",
        "\n",
        "class ProtRAP_LM():\n",
        "\n",
        "    def __init__(self,device_name='cpu'):\n",
        "        device = torch.device(device_name)\n",
        "        self.device=device\n",
        "\n",
        "        esm_model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "        esm_model=esm_model.eval().to(device)\n",
        "        models=[]\n",
        "        for i in range(10):\n",
        "            model=torch.jit.load(model_path(i)).to(device).eval()\n",
        "            models.append(model)\n",
        "        self.models=models\n",
        "        self.esm_model=esm_model\n",
        "        self.batch_converter=batch_converter\n",
        "\n",
        "    def predict(self,seq):\n",
        "        data=[('prot',seq)]\n",
        "        _, _, batch_tokens = self.batch_converter(data)\n",
        "        batch_tokens=batch_tokens.to(self.device)\n",
        "        preds=[]\n",
        "        with torch.no_grad():\n",
        "            results=self.esm_model(batch_tokens,repr_layers=[33])\n",
        "            Repr= results[\"representations\"][33]\n",
        "            for model in self.models:\n",
        "                pred=model(Repr).to(torch.device(\"cpu\"))\n",
        "                preds.append(np.array(pred[0,1:-1,:]))\n",
        "        preds=np.array(preds)\n",
        "        mean_pred=np.mean(preds,axis=0)\n",
        "        std_pred=np.std(preds,axis=0)\n",
        "        result=np.concatenate((mean_pred,std_pred),axis=-1)\n",
        "        return result\n",
        "ProtRAP_LM_model=ProtRAP_LM(device)\n",
        "fasta_str=''\n",
        "heads=[]\n",
        "seqs=[]\n",
        "sequence1=''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpTvDKMKLddT",
        "outputId": "c6925c13-eef4-4041-98ac-ae4f087869e2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ProtRAP-LM'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 61 (delta 29), reused 53 (delta 24), pack-reused 1 (from 1)\u001b[K\n",
            "Receiving objects: 100% (61/61), 73.99 KiB | 18.50 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "GPU detected. Using Tesla T4\n",
            "Downloading model_0\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_0.pts to ProtRAP-LM/models/model_0.pts\n",
            "Downloading model_1\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_1.pts to ProtRAP-LM/models/model_1.pts\n",
            "Downloading model_2\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_2.pts to ProtRAP-LM/models/model_2.pts\n",
            "Downloading model_3\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_3.pts to ProtRAP-LM/models/model_3.pts\n",
            "Downloading model_4\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_4.pts to ProtRAP-LM/models/model_4.pts\n",
            "Downloading model_5\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_5.pts to ProtRAP-LM/models/model_5.pts\n",
            "Downloading model_6\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_6.pts to ProtRAP-LM/models/model_6.pts\n",
            "Downloading model_7\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_7.pts to ProtRAP-LM/models/model_7.pts\n",
            "Downloading model_8\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_8.pts to ProtRAP-LM/models/model_8.pts\n",
            "Downloading model_9\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_9.pts to ProtRAP-LM/models/model_9.pts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are currently two options: **single input** and **multiple input**. You can choose either one.\n",
        "\n",
        "**Single Input:** Requires you to enter a sequence in the text box.\n",
        "\n",
        "**Multiple Input:** Requires you to upload a FASTA file. This file can contain multiple sequences. The program will process them together and return the results in a compressed file."
      ],
      "metadata": {
        "id": "juN3p7A51GLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Single Input\n",
        "\n",
        "\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "sequence1 = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK' #@param {type:\"string\"}\n",
        "result_name=jobname+'.zip'\n",
        "os.system(f\"rm {result_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVI1gyxzMtX9",
        "outputId": "0be6062a-7bce-4cbe-819f-e540f33afe4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Multiple Input\n",
        "\n",
        "#@markdown To avoid potential network issues, it is recommended to slice your too large fasta file into smaller portions before uploading.\n",
        "\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "uploaded = files.upload()\n",
        "\n",
        "for k in uploaded:\n",
        "  for record in SeqIO.parse(k, \"fasta\"):\n",
        "    heads.append(record.id)\n",
        "    seqs.append(str(record.seq))\n",
        "print(f'Successfully loaded a fasta file contains {len(seqs)} sequences.')\n",
        "result_name=jobname+'.zip'\n",
        "os.system(\"rm *.fasta\")"
      ],
      "metadata": {
        "id": "r306TcYSxAIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run\n",
        "run_seqs=[]\n",
        "run_heads=[]\n",
        "if len(seqs)>0:\n",
        "  run_seqs=seqs\n",
        "  run_heads=heads\n",
        "if len(sequence1)>0:\n",
        "  run_seqs.append(sequence1)\n",
        "  run_heads.append('single_input')\n",
        "results=[]\n",
        "infos=zip(run_seqs,run_heads)\n",
        "for seq,head in tqdm(infos):\n",
        "  result=ProtRAP_LM_model.predict(seq)\n",
        "  np.savetxt(head+'_result.csv',result,  header='MCP,RASA,MCP_std,RASA_std',delimiter=',')"
      ],
      "metadata": {
        "id": "tYwPKR1rM7bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Package and download results\n",
        "\n",
        "result_name='test.zip'\n",
        "os.system(f\"zip {result_name} *.csv\")\n",
        "os.system(\"rm *.csv\")\n",
        "files.download(f\"{result_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "--8CpMJoNl08",
        "outputId": "fb6622f7-e081-42c5-c585-0438632aebe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c3c5351f-1432-4dde-9d7f-e711afb3f5db\", \"test.zip\", 157261)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}