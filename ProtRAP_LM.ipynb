{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpTvDKMKLddT",
        "outputId": "22ab50e9-933a-46e6-e39e-23e8658707f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ProtRAP-LM'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 50 (delta 23), reused 47 (delta 21), pack-reused 1 (from 1)\u001b[K\n",
            "Receiving objects: 100% (50/50), 24.04 KiB | 24.04 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "No TPU or GPU detected. Using CPU.\n",
            "Downloading model_0\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_0.pts to ProtRAP-LM/models/model_0.pts\n",
            "Downloading model_1\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_1.pts to ProtRAP-LM/models/model_1.pts\n",
            "Downloading model_2\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_2.pts to ProtRAP-LM/models/model_2.pts\n",
            "Downloading model_3\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_3.pts to ProtRAP-LM/models/model_3.pts\n",
            "Downloading model_4\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_4.pts to ProtRAP-LM/models/model_4.pts\n",
            "Downloading model_5\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_5.pts to ProtRAP-LM/models/model_5.pts\n",
            "Downloading model_6\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_6.pts to ProtRAP-LM/models/model_6.pts\n",
            "Downloading model_7\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_7.pts to ProtRAP-LM/models/model_7.pts\n",
            "Downloading model_8\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_8.pts to ProtRAP-LM/models/model_8.pts\n",
            "Downloading model_9\n",
            "Downloaded file from https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_9.pts to ProtRAP-LM/models/model_9.pts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies\n",
        "!git clone https://github.com/ComputBiophys/ProtRAP-LM.git\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse,csv,sys\n",
        "import os,requests\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nnF\n",
        "import os # 用于检测 Colab TPU 环境\n",
        "\n",
        "# --- 设备检测 ---\n",
        "device = None\n",
        "device_type = None\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ and os.environ['COLAB_TPU_ADDR']:\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        device = xm.xla_device()\n",
        "        device_type = \"TPU\"\n",
        "        print(\"TPU detected. Using TPU.\")\n",
        "    except ImportError:\n",
        "        print(\"TPU environment detected, but torch_xla is not installed.\")\n",
        "        print(\"Falling back to CPU/GPU check.\")\n",
        "if device is None:\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        device_type = \"GPU\"\n",
        "        print(f\"GPU detected. Using {torch.cuda.get_device_name(0)}\") # 显示 GPU 名称\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_type = \"CPU\"\n",
        "        print(\"No TPU or GPU detected. Using CPU.\")\n",
        "\n",
        "model_path=lambda x:'ProtRAP-LM/models/model_'+str(x)+'.pts'\n",
        "github_url=lambda x:f\"https://github.com/ComputBiophys/ProtRAP-LM/releases/download/Version1.0/model_{str(x)}.pts\"\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded file from {url} to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}, You may manually download this one\")\n",
        "\n",
        "for i in range(10):\n",
        "    if not os.path.exists(model_path(i)):\n",
        "        print('Downloading model_'+str(i))\n",
        "        download_file(github_url(i), model_path(i))\n",
        "\n",
        "def fasta_load(fasta_dir):\n",
        "    fp = open(fasta_dir, 'r')\n",
        "    lines = fp.readlines()\n",
        "    fp.close()\n",
        "    sequence = ''\n",
        "    for line in lines[1:]:\n",
        "        sequence = sequence + line.split()[0]\n",
        "    return sequence\n",
        "def weight_MSE_loss(labels,logits,weights=1):\n",
        "    l=(labels-logits)**2\n",
        "    l=l*weights\n",
        "    return torch.sum(l)\n",
        "def focal_loss_softmax(labels,logits):\n",
        "    y_pred=logits\n",
        "    l=-labels*torch.log(y_pred+1e-8)*((1-y_pred)**2)\n",
        "    return torch.sum(l)\n",
        "\n",
        "class MultiScaleCNN(nn.Module):\n",
        "    def __init__(self,input_dim=1280,output_dim=256):#,size=[3,7,11],padding=[1,3,5]):\n",
        "        super().__init__()\n",
        "        self.cnn1=nn.Conv1d(input_dim,output_dim,3,padding=1)\n",
        "        self.cnn2=nn.Conv1d(input_dim,output_dim,5,padding=2)\n",
        "        self.cnn3=nn.Conv1d(input_dim,output_dim,7,padding=3)\n",
        "        self.cnn4=nn.Conv1d(input_dim,output_dim,9,padding=4)\n",
        "    def forward(self,x):\n",
        "        x=x.permute(0,2,1)\n",
        "        x1=self.cnn1(x)\n",
        "        x2=self.cnn2(x)\n",
        "        x3=self.cnn3(x)\n",
        "        x4=self.cnn4(x)\n",
        "        x=torch.cat((x1,x2,x3,x4), -2)\n",
        "        x=x.permute(0,2,1)\n",
        "        return x\n",
        "\n",
        "class ProtRAP_LM():\n",
        "\n",
        "    def __init__(self,device_name='cpu'):\n",
        "        device = torch.device(device_name)\n",
        "        self.device=device\n",
        "\n",
        "        esm_model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "        esm_model=esm_model.eval().to(device)\n",
        "        models=[]\n",
        "        for i in range(10):\n",
        "            model=torch.jit.load(model_path(i)).to(device).eval()\n",
        "            models.append(model)\n",
        "        self.models=models\n",
        "        self.esm_model=esm_model\n",
        "        self.batch_converter=batch_converter\n",
        "\n",
        "    def predict(self,seq):\n",
        "        data=[('prot',seq)]\n",
        "        _, _, batch_tokens = self.batch_converter(data)\n",
        "        batch_tokens=batch_tokens.to(self.device)\n",
        "        preds=[]\n",
        "        with torch.no_grad():\n",
        "            results=self.esm_model(batch_tokens,repr_layers=[33])\n",
        "            Repr= results[\"representations\"][33]\n",
        "            for model in self.models:\n",
        "                pred=model(Repr).to(torch.device(\"cpu\"))\n",
        "                preds.append(np.array(pred[0,1:-1,:]))\n",
        "        preds=np.array(preds)\n",
        "        mean_pred=np.mean(preds,axis=0)\n",
        "        std_pred=np.std(preds,axis=0)\n",
        "        result=np.concatenate((mean_pred,std_pred),axis=-1)\n",
        "        return result\n",
        "ProtRAP_LM_model=ProtRAP_LM(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVI1gyxzMtX9",
        "outputId": "0be6062a-7bce-4cbe-819f-e540f33afe4b"
      },
      "outputs": [],
      "source": [
        "#@title Input\n",
        "\n",
        "\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK' #@param {type:\"string\"}\n",
        "result_name=jobname+'.zip'\n",
        "os.system(f\"rm {result_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "tYwPKR1rM7bW"
      },
      "outputs": [],
      "source": [
        "#@title Run\n",
        "result=ProtRAP_LM_model.predict(sequence)\n",
        "np.savetxt(jobname+'_result.csv', result, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "NBCg-BXLPIsE",
        "outputId": "02c59ba0-1c99-4174-a0cf-7179c584d2d9"
      },
      "outputs": [],
      "source": [
        "#@title Plots {run: \"auto\"}\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mean=result[:,:2]\n",
        "std=result[:,2:]\n",
        "\n",
        "y_lower= mean-std\n",
        "y_upper = mean+std\n",
        "\n",
        "x=np.arange(mean.shape[0])\n",
        "\n",
        "plt.fill_between(x, y_lower[:,0], y_upper[:,0], color=\"red\", alpha=0.2)\n",
        "plt.plot(x,mean[:,0],color=\"red\", linewidth=1,label ='MCP')\n",
        "\n",
        "plt.fill_between(x, y_lower[:,1], y_upper[:,1], color=\"blue\", alpha=0.2)\n",
        "plt.plot(x,mean[:,1],color=\"blue\", linewidth=1,label='RASA')\n",
        "plt.ylim(0,1)\n",
        "plt.xlim(0,mean.shape[0])\n",
        "plt.xlabel('sequence')\n",
        "plt.ylabel('RA value')\n",
        "plt.legend()\n",
        "plt.savefig(jobname+'_plot.png',dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "--8CpMJoNl08",
        "outputId": "f1d9763c-3eb7-4155-fb8c-3a693c51aee5"
      },
      "outputs": [],
      "source": [
        "#@title Package and download results\n",
        "from google.colab import drive,files\n",
        "os.system(f\"zip {result_name} {jobname}_*\")\n",
        "os.system(f\"rm {jobname}_*\")\n",
        "files.download(f\"{result_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
